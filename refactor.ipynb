{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Modules\n",
    "from model.vae import Encoder, Decoder, AutoencoderKL, SDPatchGANDiscriminator\n",
    "from model.unet import UNetModelSmall\n",
    "from model.text_encoder import TextEncoderWrapper\n",
    "import input_pipeline\n",
    "from trainer import VAETrainer, LatentDiffusionTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Hardware Setup\n",
    "def setup_hardware():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Restrict TensorFlow to only use the fourth GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "setup_hardware()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './dataset'\n",
    "VOCAB = '/dictionary/id2Word.npy'\n",
    "WORD2ID = '/dictionary/word2Id.npy'\n",
    "ID2WORD = '/dictionary/id2Word.npy'\n",
    "TRAIN = '/dataset/text2ImgData.pkl'\n",
    "TEST = '/dataset/testData.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = ROOT+TRAIN\n",
    "vocab_path = ROOT+VOCAB\n",
    "word2_id_path = ROOT+WORD2ID\n",
    "id2_word_path = ROOT+ID2WORD\n",
    "save_embeding_path = \"./seqemb/seq_emb_multi.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5429 vocabularies in total\n"
     ]
    }
   ],
   "source": [
    "# Load Dictionaries (Global as they are small and needed everywhere)\n",
    "try:\n",
    "    vocab = np.load(vocab_path)\n",
    "    print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "    word2Id_dict = dict(np.load(word2_id_path))\n",
    "    id2word_dict = dict(np.load(id2_word_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Dictionaries not found. Proceeding assuming they might not be needed immediately if re-generating embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing():\n",
    "    \"\"\"Run text preprocessing using CLIP. This function scopes the TextEncoderWrapper \n",
    "    so it can be garbage collected after use.\"\"\"\n",
    "\n",
    "    print(\"\\n[Preprocessing] Checking if embeddings need to be generated...\")\n",
    "    # Only run if you need to generate embeddings\n",
    "    # preprocess_captions_all(train_path, save_embeding_path, max_caption_len=5)\n",
    "    \n",
    "    # --- 修改開始：把下面這段會報錯的測試代碼全部註解掉 ---\n",
    "    # print(\"[Preprocessing] Loading Text Encoder for testing...\")\n",
    "    # text_encoder = TextEncoderWrapper()\n",
    "    \n",
    "    # sample = ['9', '1', '82', '5', '11', '70', '20', '31', \n",
    "    #           '3', '29', '20', '2', '5427', '5427', '5427', '5427', \n",
    "    #           '5427', '5427', '5427', '5427']\n",
    "    # # 注意：id2word_dict 需要是全局變數或傳入\n",
    "    # sample_emb = text_encoder.id2clip(sample, id2word_dict)\n",
    "    # print(\"Sample embedding shape:\", sample_emb.shape)\n",
    "    \n",
    "    # # Explicitly delete to be safe, though function scope handles it\n",
    "    # del text_encoder\n",
    "    # gc.collect()\n",
    "    # tf.keras.backend.clear_session()\n",
    "    # print(\"[Preprocessing] Text Encoder memory released.\\n\")\n",
    "    # --- 修改結束 ---\n",
    "    \n",
    "    print(\"[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae_models():\n",
    "    \"\"\"Load and return VAE (Encoder/Decoder) with weights loaded.\"\"\"\n",
    "    decoder_weights_fpath = keras.utils.get_file(\n",
    "                origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\",\n",
    "                file_hash=\"6d3c5ba91d5cc2b134da881aaa157b2d2adc648e5625560e3ed199561d0e39d5\",\n",
    "            )\n",
    "\n",
    "    encoder_weights_fpath = keras.utils.get_file(\n",
    "        origin=\"https://huggingface.co/divamgupta/stable-diffusion-tensorflow/resolve/main/encoder_newW.h5\",\n",
    "        file_hash=\"56a2578423c640746c5e90c0a789b9b11481f47497f817e65b44a1a5538af754\",\n",
    "    )\n",
    "\n",
    "    print(\"[VAE] Loading Decoder...\")\n",
    "    decoder = Decoder()\n",
    "    latent = keras.layers.Input((16,16,4))\n",
    "    decoder_model = keras.models.Model(latent, decoder(latent))\n",
    "    decoder_model.load_weights(decoder_weights_fpath)\n",
    "\n",
    "    print(\"[VAE] Loading Encoder...\")\n",
    "    encoder = Encoder()\n",
    "    inp_img = keras.layers.Input((128,128,3))\n",
    "    encoder_model = keras.models.Model(inp_img, encoder(inp_img))\n",
    "    encoder_model.load_weights(encoder_weights_fpath)\n",
    "    \n",
    "    # Return inner models directly or wrapped ones depending on use\n",
    "    # trainer.py uses 'vae' which wraps inner encoder/decoder\n",
    "    # but LatentDiffusionTrainer uses vae.encoder / vae.decoder\n",
    "    \n",
    "    vae = AutoencoderKL(encoder, decoder, latent_channels=4, scaling_factor=0.18215)\n",
    "    return vae, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vae_training(vae, BATCH_SIZE=16):\n",
    "    \"\"\"Run VAE Training. Separation allows keeping Discriminator/LPIPS ONLY in this scope.\"\"\"\n",
    "    print(\"\\n[VAE Training] Initializing Discriminator and Trainer...\")\n",
    "    \n",
    "    discriminator = SDPatchGANDiscriminator(\n",
    "        input_channels=3,\n",
    "        base_filters=64,\n",
    "        n_layers=3\n",
    "    )\n",
    "    \n",
    "    h_disc_start = 0 # Example config\n",
    "    trainer = VAETrainer(\n",
    "        vae, \n",
    "        discriminator,\n",
    "        disc_start=h_disc_start,\n",
    "        kl_weight=1e-6,\n",
    "        perc_weight=0.1,\n",
    "        disc_weight=0.05\n",
    "    ) # This loads VGG19 inside!\n",
    "\n",
    "    base_lr = 4.5e-6\n",
    "    lr = base_lr * BATCH_SIZE\n",
    "    vae_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "    disc_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    trainer.compile(vae_optimizer=vae_opt, disc_optimizer=disc_opt)\n",
    "    \n",
    "    # ... Training loop would go here ...\n",
    "    print(\"[VAE Training] Done (Placeholder). Clearing memory...\")\n",
    "    \n",
    "    # Cleanup heavy training objects\n",
    "    del trainer\n",
    "    del discriminator\n",
    "    del vae_opt\n",
    "    del disc_opt\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"[VAE Training] Memory cleared.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_training(batch_size = 32, total_epochs = 50, continue_training = False):\n",
    "    \"\"\"Main Diffusion Training Loop\"\"\"\n",
    "    \n",
    "    # 1. Preprocessing (Optional, ensures clean state)\n",
    "    run_preprocessing()\n",
    "    \n",
    "    # 2. Load Dataset\n",
    "    print(\"[Main] Loading Dataset...\")\n",
    "    dataset, train_data, valid_data = input_pipeline.dataset_generator(\n",
    "        train_path, batch_size, 0.8, save_embeding_path, ROOT\n",
    "    )\n",
    "    \n",
    "    # 3. Load VAE (Needed for Diffusion Training execution - encoding images)\n",
    "    print(\"[Main] Loading VAE for Diffusion...\")\n",
    "    vae, encoder_model, decoder_model = get_vae_models()\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.trainable = False\n",
    "    encoder_model.trainable = False\n",
    "    decoder_model.trainable = False\n",
    "    \n",
    "    # 4. Initialize UNet\n",
    "    print(\"[Main] Initializing UNet...\")\n",
    "    unet = UNetModelSmall()\n",
    "    ema_unet = UNetModelSmall()\n",
    "\n",
    "    # Build UNet\n",
    "    x_in = keras.Input(shape=(16, 16, 4))\n",
    "    t_in = keras.Input(shape=(512,))\n",
    "    c_in = keras.Input(shape=(77, 768))\n",
    "    unet([x_in, t_in, c_in])\n",
    "    ema_unet([x_in, t_in, c_in])\n",
    "    \n",
    "    print(\"[Main] Copying EMA weights...\")\n",
    "    ema_unet.set_weights(unet.get_weights())\n",
    "    \n",
    "    # 5. Initialize Diffusion Trainer\n",
    "    # Note: We pass encoder_model/decoder_model (the wrapped Keras models) or the raw layers?\n",
    "    # Checking trainer.py: self.vae_encoder(images) -> expects Keras Model or Layer\n",
    "    diffusion_model = LatentDiffusionTrainer(unet, ema_unet, encoder_model, decoder_model)\n",
    "    \n",
    "    EPOCH_STEPS = 460 # Adjust based on dataset size\n",
    "    TOTAL_STEPS = total_epochs * EPOCH_STEPS\n",
    "    WARMUP_RATIO = 0.1\n",
    "    WARMUP_STEPS = int(TOTAL_STEPS * WARMUP_RATIO)\n",
    "    INITIAL_LR = 1e-4\n",
    "\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=INITIAL_LR,\n",
    "        decay_steps=TOTAL_STEPS, \n",
    "        warmup_steps=WARMUP_STEPS\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule, \n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    diffusion_model.compile(optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    checkpoint_path = f\"./checkpoints/tf_checkpoint.weights.h5\"\n",
    "\n",
    "    # 檢查並載入權重\n",
    "    if continue_training and os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint at {checkpoint_path}, loading weights to resume training...\")\n",
    "        \n",
    "        # 強制 Build KID\n",
    "        try:\n",
    "            print(\"Force building KID layer to match checkpoint structure...\")\n",
    "            # 建立假圖片數據 (Batch=1, Size=128x128, Channels=3)\n",
    "            dummy_img = tf.zeros((1, 128, 128, 3))\n",
    "            # 戳一下 update_state，強迫 InceptionV3 初始化權重\n",
    "            diffusion_model.kid.update_state(dummy_img, dummy_img)\n",
    "            print(\"KID layer built successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to force build KID: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # 因為是 save_weights_only=True，所以用 load_weights\n",
    "            diffusion_model.load_weights(checkpoint_path, skip_mismatch=True)\n",
    "            print(\"Weights loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load weights: {e}\")\n",
    "            print(\"Starting training from scratch.\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_kid\", # Monitor KID metric\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "    # 新的條件式繪圖 Callback\n",
    "    def conditional_plot(epoch, logs):\n",
    "        # 從 logs 取得驗證集的 KID 分數\n",
    "        # 注意：Keras 的驗證集 metric 通常會加上 \"val_\" 前綴\n",
    "        current_kid = logs.get(\"val_kid\")\n",
    "        \n",
    "        # 打印當前 KID 方便確認\n",
    "        print(f\"\\nEpoch {epoch + 1}: val_kid = {current_kid:.4f}\")\n",
    "\n",
    "        # 設定你的閾值 (例如 < 1.0 或更低)\n",
    "        TARGET_KID = 0.8 \n",
    "\n",
    "        if current_kid is not None and current_kid < TARGET_KID:\n",
    "            diffusion_model.plot_images(valid_data)\n",
    "        else:\n",
    "            print(f\"KID ({current_kid:.4f}) still high (>= {TARGET_KID}), skipping generation to save time.\")\n",
    "\n",
    "    plot_cb = keras.callbacks.LambdaCallback(on_epoch_end=conditional_plot)\n",
    "\n",
    "    print(\"[Main] Starting Diffusion Training...\")\n",
    "    diffusion_model.fit(\n",
    "        dataset, \n",
    "        validation_data=valid_data,\n",
    "        epochs=total_epochs,\n",
    "        callbacks=[plot_cb, checkpoint_callback],\n",
    "        verbose=2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Preprocessing] Checking if embeddings need to be generated...\n",
      "[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\n",
      "[Main] Loading Dataset...\n",
      "[Main] Loading VAE for Diffusion...\n",
      "[VAE] Loading Decoder...\n",
      "[VAE] Loading Encoder...\n",
      "[Main] Initializing UNet...\n",
      "[Main] Copying EMA weights...\n",
      "Found checkpoint at ./checkpoints/tf_checkpoint.weights.h5, loading weights to resume training...\n",
      "Force building KID layer to match checkpoint structure...\n",
      "KID layer built successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'AdamW', because it has 1 variables whereas the saved optimizer has 1373 variables. \n",
      "/venv/main/lib/python3.10/site-packages/tf_keras/src/saving/saving_lib.py:521: UserWarning: Could not load weights in object KID(name=kid,dtype=float32). Skipping object. Exception encountered: Layer 'kid' expected 2 variables, but received 4 variables during loading. Expected: ['total:0', 'count:0']\n",
      "  _load_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded successfully!\n",
      "[Main] Starting Diffusion Training...\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_kid = 1.0384\n",
      "KID (1.0384) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 667s - n_loss: 0.1799 - i_loss: 0.3496 - val_n_loss: 0.1719 - val_i_loss: 0.2927 - val_kid: 1.0384 - 667s/epoch - 3s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_kid = 0.9499\n",
      "KID (0.9499) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 103s - n_loss: 0.1765 - i_loss: 0.3288 - val_n_loss: 0.1647 - val_i_loss: 0.2898 - val_kid: 0.9499 - 103s/epoch - 447ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_kid = 0.9887\n",
      "KID (0.9887) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 100s - n_loss: 0.1755 - i_loss: 0.3128 - val_n_loss: 0.1628 - val_i_loss: 0.2805 - val_kid: 0.9887 - 100s/epoch - 436ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_kid = 0.8899\n",
      "KID (0.8899) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 101s - n_loss: 0.1682 - i_loss: 0.3243 - val_n_loss: 0.1541 - val_i_loss: 0.2899 - val_kid: 0.8899 - 101s/epoch - 438ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_kid = 1.0072\n",
      "KID (1.0072) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1665 - i_loss: 0.3260 - val_n_loss: 0.1548 - val_i_loss: 0.2663 - val_kid: 1.0072 - 97s/epoch - 422ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_kid = 1.0299\n",
      "KID (1.0299) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1639 - i_loss: 0.3128 - val_n_loss: 0.1540 - val_i_loss: 0.2686 - val_kid: 1.0299 - 98s/epoch - 425ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_kid = 0.9840\n",
      "KID (0.9840) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1647 - i_loss: 0.3046 - val_n_loss: 0.1514 - val_i_loss: 0.2623 - val_kid: 0.9840 - 96s/epoch - 415ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_kid = 0.9945\n",
      "KID (0.9945) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1615 - i_loss: 0.3085 - val_n_loss: 0.1480 - val_i_loss: 0.2643 - val_kid: 0.9945 - 96s/epoch - 419ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_kid = 0.9781\n",
      "KID (0.9781) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1554 - i_loss: 0.2977 - val_n_loss: 0.1448 - val_i_loss: 0.2517 - val_kid: 0.9781 - 98s/epoch - 427ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_kid = 0.9455\n",
      "KID (0.9455) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1552 - i_loss: 0.2907 - val_n_loss: 0.1419 - val_i_loss: 0.2414 - val_kid: 0.9455 - 96s/epoch - 416ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_kid = 1.0007\n",
      "KID (1.0007) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1540 - i_loss: 0.2833 - val_n_loss: 0.1405 - val_i_loss: 0.2443 - val_kid: 1.0007 - 98s/epoch - 426ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_kid = 1.0326\n",
      "KID (1.0326) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 99s - n_loss: 0.1518 - i_loss: 0.2822 - val_n_loss: 0.1383 - val_i_loss: 0.2410 - val_kid: 1.0326 - 99s/epoch - 429ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_kid = 1.0322\n",
      "KID (1.0322) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 102s - n_loss: 0.1497 - i_loss: 0.2718 - val_n_loss: 0.1376 - val_i_loss: 0.2296 - val_kid: 1.0322 - 102s/epoch - 442ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_kid = 1.0371\n",
      "KID (1.0371) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1429 - i_loss: 0.2807 - val_n_loss: 0.1233 - val_i_loss: 0.2368 - val_kid: 1.0371 - 98s/epoch - 425ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_kid = 1.0727\n",
      "KID (1.0727) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 99s - n_loss: 0.1429 - i_loss: 0.2680 - val_n_loss: 0.1258 - val_i_loss: 0.2249 - val_kid: 1.0727 - 99s/epoch - 430ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_kid = 1.0002\n",
      "KID (1.0002) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 95s - n_loss: 0.1424 - i_loss: 0.2552 - val_n_loss: 0.1288 - val_i_loss: 0.2190 - val_kid: 1.0002 - 95s/epoch - 415ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_kid = 0.9315\n",
      "KID (0.9315) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1360 - i_loss: 0.2602 - val_n_loss: 0.1231 - val_i_loss: 0.1982 - val_kid: 0.9315 - 97s/epoch - 420ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_kid = 0.9129\n",
      "KID (0.9129) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 99s - n_loss: 0.1327 - i_loss: 0.2537 - val_n_loss: 0.1215 - val_i_loss: 0.2141 - val_kid: 0.9129 - 99s/epoch - 429ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_kid = 0.9690\n",
      "KID (0.9690) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1316 - i_loss: 0.2491 - val_n_loss: 0.1190 - val_i_loss: 0.2018 - val_kid: 0.9690 - 96s/epoch - 419ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_kid = 1.0046\n",
      "KID (1.0046) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1263 - i_loss: 0.2495 - val_n_loss: 0.1111 - val_i_loss: 0.2032 - val_kid: 1.0046 - 97s/epoch - 422ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_kid = 0.9575\n",
      "KID (0.9575) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1227 - i_loss: 0.2376 - val_n_loss: 0.1116 - val_i_loss: 0.1944 - val_kid: 0.9575 - 98s/epoch - 427ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_kid = 1.0546\n",
      "KID (1.0546) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1216 - i_loss: 0.2420 - val_n_loss: 0.1068 - val_i_loss: 0.1887 - val_kid: 1.0546 - 97s/epoch - 421ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_kid = 1.0859\n",
      "KID (1.0859) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1187 - i_loss: 0.2346 - val_n_loss: 0.1014 - val_i_loss: 0.1953 - val_kid: 1.0859 - 97s/epoch - 423ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_kid = 1.0258\n",
      "KID (1.0258) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1203 - i_loss: 0.2205 - val_n_loss: 0.1024 - val_i_loss: 0.1913 - val_kid: 1.0258 - 98s/epoch - 428ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_kid = 0.9506\n",
      "KID (0.9506) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1151 - i_loss: 0.2188 - val_n_loss: 0.1020 - val_i_loss: 0.1745 - val_kid: 0.9506 - 96s/epoch - 415ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_kid = 0.9983\n",
      "KID (0.9983) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 96s - n_loss: 0.1127 - i_loss: 0.2193 - val_n_loss: 0.0995 - val_i_loss: 0.1743 - val_kid: 0.9983 - 96s/epoch - 418ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_kid = 0.9209\n",
      "KID (0.9209) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1079 - i_loss: 0.2144 - val_n_loss: 0.0958 - val_i_loss: 0.1762 - val_kid: 0.9209 - 98s/epoch - 426ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_kid = 0.8788\n",
      "KID (0.8788) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 102s - n_loss: 0.1080 - i_loss: 0.2062 - val_n_loss: 0.0860 - val_i_loss: 0.1868 - val_kid: 0.8788 - 102s/epoch - 442ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_kid = 0.9307\n",
      "KID (0.9307) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 98s - n_loss: 0.1042 - i_loss: 0.2001 - val_n_loss: 0.0884 - val_i_loss: 0.1612 - val_kid: 0.9307 - 98s/epoch - 425ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_kid = 0.8895\n",
      "KID (0.8895) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.1019 - i_loss: 0.1985 - val_n_loss: 0.0842 - val_i_loss: 0.1787 - val_kid: 0.8895 - 97s/epoch - 423ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_kid = 0.8492\n",
      "KID (0.8492) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 101s - n_loss: 0.1003 - i_loss: 0.1975 - val_n_loss: 0.0821 - val_i_loss: 0.1668 - val_kid: 0.8492 - 101s/epoch - 437ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_kid = 0.8495\n",
      "KID (0.8495) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 - 97s - n_loss: 0.0963 - i_loss: 0.1906 - val_n_loss: 0.0821 - val_i_loss: 0.1570 - val_kid: 0.8495 - 97s/epoch - 423ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_kid = 0.8289\n",
      "KID (0.8289) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 - 103s - n_loss: 0.0937 - i_loss: 0.1908 - val_n_loss: 0.0816 - val_i_loss: 0.1524 - val_kid: 0.8289 - 103s/epoch - 446ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_kid = 0.7631\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'cross_attention_161' (type CrossAttention).\n\nCannot do batch_dot on inputs with different batch sizes. Received inputs with shapes (128, 256, 16) and (256, 16, 77).\n\nCall arguments received by layer 'cross_attention_161' (type CrossAttention):\n  • inputs=['tf.Tensor(shape=(16, 256, 128), dtype=float32)', 'tf.Tensor(shape=(32, 77, 768), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m TOTAL_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the main pipeline\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mrun_diffusion_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 120\u001b[0m, in \u001b[0;36mrun_diffusion_training\u001b[0;34m(batch_size, total_epochs, continue_training)\u001b[0m\n\u001b[1;32m    117\u001b[0m plot_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mLambdaCallback(on_epoch_end\u001b[38;5;241m=\u001b[39mconditional_plot)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Main] Starting Diffusion Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mplot_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[12], line 113\u001b[0m, in \u001b[0;36mrun_diffusion_training.<locals>.conditional_plot\u001b[0;34m(epoch, logs)\u001b[0m\n\u001b[1;32m    110\u001b[0m TARGET_KID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_kid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m current_kid \u001b[38;5;241m<\u001b[39m TARGET_KID:\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mdiffusion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKID (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_kid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) still high (>= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_KID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), skipping generation to save time.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/cup3/trainer.py:408\u001b[0m, in \u001b[0;36mLatentDiffusionTrainer.plot_images\u001b[0;34m(self, valid_data, draw_diffusion_steps, num_rows, num_cols, figsize)\u001b[0m\n\u001b[1;32m    406\u001b[0m val_seq_emb_batch,_\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(valid_data))\n\u001b[1;32m    407\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m num_rows \u001b[38;5;241m*\u001b[39m num_cols\n\u001b[0;32m--> 408\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_diffusion_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq_emb_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m generated_samples\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    410\u001b[0m total_imgs \u001b[38;5;241m=\u001b[39m generated_samples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspace/cup3/trainer.py:355\u001b[0m, in \u001b[0;36mLatentDiffusionTrainer.generate_images\u001b[0;34m(self, batch_size, diffusion_steps, seq_emb)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size, diffusion_steps, seq_emb):\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# 隨機初始 latent\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     noisy_latents \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m    353\u001b[0m         shape\u001b[38;5;241m=\u001b[39m(batch_size, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    354\u001b[0m     )\n\u001b[0;32m--> 355\u001b[0m     pred_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     pred_latents \u001b[38;5;241m=\u001b[39m pred_latents \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_scale\n\u001b[1;32m    357\u001b[0m     decoded_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_decoder(pred_latents, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Changed to False because no global EMA var\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/cup3/trainer.py:292\u001b[0m, in \u001b[0;36mLatentDiffusionTrainer.reverse_diffusion\u001b[0;34m(self, initial_noise, diffusion_steps, seq_emb)\u001b[0m\n\u001b[1;32m    289\u001b[0m     next_noise_rates, next_signal_rates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_schedule(next_diffusion_times)\n\u001b[1;32m    290\u001b[0m     t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_embedding(timesteps)\n\u001b[0;32m--> 292\u001b[0m     pred_noises, pred_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_noisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_rates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     next_noisy_images \u001b[38;5;241m=\u001b[39m next_signal_rates \u001b[38;5;241m*\u001b[39m pred_images \u001b[38;5;241m+\u001b[39m next_noise_rates \u001b[38;5;241m*\u001b[39m pred_noises\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_images\n",
      "File \u001b[0;32m/workspace/cup3/trainer.py:265\u001b[0m, in \u001b[0;36mLatentDiffusionTrainer.denoise\u001b[0;34m(self, noisy_images, t_emb, seq_emb, noise_rates, signal_rates, training)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema_unet\n\u001b[0;32m--> 265\u001b[0m pred_noises \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m pred_images \u001b[38;5;241m=\u001b[39m (noisy_images \u001b[38;5;241m-\u001b[39m noise_rates\u001b[38;5;241m*\u001b[39mpred_noises)\u001b[38;5;241m/\u001b[39msignal_rates\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_noises, pred_images\n",
      "File \u001b[0;32m/workspace/cup3/model/unet.py:283\u001b[0m, in \u001b[0;36mUNetModelSmall.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m block:\n\u001b[0;32m--> 283\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mapply_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     saved_inputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# ---------- Middle ----------\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/cup3/model/unet.py:273\u001b[0m, in \u001b[0;36mUNetModelSmall.call.<locals>.apply_layer\u001b[0;34m(x, layer)\u001b[0m\n\u001b[1;32m    271\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer([x, emb])\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m--> 273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m/workspace/cup3/model/unet.py:111\u001b[0m, in \u001b[0;36mSpatialTransformer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(x, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, h \u001b[38;5;241m*\u001b[39m w, c))\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(x, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, h, w, c))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x) \u001b[38;5;241m+\u001b[39m x_in\n",
      "File \u001b[0;32m/workspace/cup3/model/unet.py:90\u001b[0m, in \u001b[0;36mBasicTransformerBlock.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     88\u001b[0m x, context \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     89\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)]) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m---> 90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeglu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/workspace/cup3/model/unet.py:64\u001b[0m, in \u001b[0;36mCrossAttention.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     61\u001b[0m k \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mPermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))(k)  \u001b[38;5;66;03m# (bs, num_heads, head_size, time)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m v \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mPermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))(v)  \u001b[38;5;66;03m# (bs, num_heads, time, head_size)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtd_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m     65\u001b[0m weights \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39msoftmax(score)  \u001b[38;5;66;03m# (bs, num_heads, time, time)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m attention \u001b[38;5;241m=\u001b[39m td_dot(weights, v)\n",
      "File \u001b[0;32m/workspace/cup3/model/common.py:46\u001b[0m, in \u001b[0;36mtd_dot\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     44\u001b[0m aa \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(a, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m     45\u001b[0m bb \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(b, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m---> 46\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43maa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreshape(cc, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], cc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], cc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'cross_attention_161' (type CrossAttention).\n\nCannot do batch_dot on inputs with different batch sizes. Received inputs with shapes (128, 256, 16) and (256, 16, 77).\n\nCall arguments received by layer 'cross_attention_161' (type CrossAttention):\n  • inputs=['tf.Tensor(shape=(16, 256, 128), dtype=float32)', 'tf.Tensor(shape=(32, 77, 768), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prevent memory fragmentation\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    TOTAL_EPOCHS = 50\n",
    "    # Run the main pipeline\n",
    "    run_diffusion_training(BATCH_SIZE, TOTAL_EPOCHS, continue_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
