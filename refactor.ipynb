{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Modules\n",
    "from model.vae import Encoder, Decoder, AutoencoderKL, SDPatchGANDiscriminator\n",
    "from model.unet import UNetModelSmall\n",
    "from model.text_encoder import TextEncoderWrapper\n",
    "import input_pipeline\n",
    "from trainer import VAETrainer, LatentDiffusionTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware Setup\n",
    "def setup_hardware():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Restrict TensorFlow to only use the fourth GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "setup_hardware()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './dataset'\n",
    "VOCAB = '/dictionary/id2Word.npy'\n",
    "WORD2ID = '/dictionary/word2Id.npy'\n",
    "ID2WORD = '/dictionary/id2Word.npy'\n",
    "TRAIN = '/dataset/text2ImgData.pkl'\n",
    "TEST = '/dataset/testData.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = ROOT+TRAIN\n",
    "vocab_path = ROOT+VOCAB\n",
    "word2_id_path = ROOT+WORD2ID\n",
    "id2_word_path = ROOT+ID2WORD\n",
    "save_embeding_path = \"./seqemb/seq_emb_multi.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dictionaries (Global as they are small and needed everywhere)\n",
    "try:\n",
    "    vocab = np.load(vocab_path)\n",
    "    print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "    word2Id_dict = dict(np.load(word2_id_path))\n",
    "    id2word_dict = dict(np.load(id2_word_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Dictionaries not found. Proceeding assuming they might not be needed immediately if re-generating embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing():\n",
    "    \"\"\"Run text preprocessing using CLIP. This function scopes the TextEncoderWrapper \n",
    "    so it can be garbage collected after use.\"\"\"\n",
    "\n",
    "    print(\"\\n[Preprocessing] Checking if embeddings need to be generated...\")\n",
    "    # Only run if you need to generate embeddings\n",
    "    # preprocess_captions_all(train_path, save_embeding_path, max_caption_len=5)\n",
    "    \n",
    "    # --- ä¿®æ”¹é–‹å§‹ï¼šæŠŠä¸‹é¢é€™æ®µæœƒå ±éŒ¯çš„æ¸¬è©¦ä»£ç¢¼å…¨éƒ¨è¨»è§£æ‰ ---\n",
    "    # print(\"[Preprocessing] Loading Text Encoder for testing...\")\n",
    "    # text_encoder = TextEncoderWrapper()\n",
    "    \n",
    "    # sample = ['9', '1', '82', '5', '11', '70', '20', '31', \n",
    "    #           '3', '29', '20', '2', '5427', '5427', '5427', '5427', \n",
    "    #           '5427', '5427', '5427', '5427']\n",
    "    # # æ³¨æ„ï¼šid2word_dict éœ€è¦æ˜¯å…¨å±€è®Šæ•¸æˆ–å‚³å…¥\n",
    "    # sample_emb = text_encoder.id2clip(sample, id2word_dict)\n",
    "    # print(\"Sample embedding shape:\", sample_emb.shape)\n",
    "    \n",
    "    # # Explicitly delete to be safe, though function scope handles it\n",
    "    # del text_encoder\n",
    "    # gc.collect()\n",
    "    # tf.keras.backend.clear_session()\n",
    "    # print(\"[Preprocessing] Text Encoder memory released.\\n\")\n",
    "    # --- ä¿®æ”¹çµæŸ ---\n",
    "    \n",
    "    print(\"[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae_models():\n",
    "    \"\"\"Load and return VAE (Encoder/Decoder) with weights loaded.\"\"\"\n",
    "    decoder_weights_fpath = keras.utils.get_file(\n",
    "                origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\",\n",
    "                file_hash=\"6d3c5ba91d5cc2b134da881aaa157b2d2adc648e5625560e3ed199561d0e39d5\",\n",
    "            )\n",
    "\n",
    "    encoder_weights_fpath = keras.utils.get_file(\n",
    "        origin=\"https://huggingface.co/divamgupta/stable-diffusion-tensorflow/resolve/main/encoder_newW.h5\",\n",
    "        file_hash=\"56a2578423c640746c5e90c0a789b9b11481f47497f817e65b44a1a5538af754\",\n",
    "    )\n",
    "\n",
    "    print(\"[VAE] Loading Decoder...\")\n",
    "    decoder = Decoder()\n",
    "    latent = keras.layers.Input((16,16,4))\n",
    "    decoder_model = keras.models.Model(latent, decoder(latent))\n",
    "    decoder_model.load_weights(decoder_weights_fpath)\n",
    "\n",
    "    print(\"[VAE] Loading Encoder...\")\n",
    "    encoder = Encoder()\n",
    "    inp_img = keras.layers.Input((128,128,3))\n",
    "    encoder_model = keras.models.Model(inp_img, encoder(inp_img))\n",
    "    encoder_model.load_weights(encoder_weights_fpath)\n",
    "    \n",
    "    # Return inner models directly or wrapped ones depending on use\n",
    "    # trainer.py uses 'vae' which wraps inner encoder/decoder\n",
    "    # but LatentDiffusionTrainer uses vae.encoder / vae.decoder\n",
    "    \n",
    "    vae = AutoencoderKL(encoder, decoder, latent_channels=4, scaling_factor=0.18215)\n",
    "    return vae, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vae_training(vae, BATCH_SIZE=16):\n",
    "    \"\"\"Run VAE Training. Separation allows keeping Discriminator/LPIPS ONLY in this scope.\"\"\"\n",
    "    print(\"\\n[VAE Training] Initializing Discriminator and Trainer...\")\n",
    "    \n",
    "    discriminator = SDPatchGANDiscriminator(\n",
    "        input_channels=3,\n",
    "        base_filters=64,\n",
    "        n_layers=3\n",
    "    )\n",
    "    \n",
    "    h_disc_start = 0 # Example config\n",
    "    trainer = VAETrainer(\n",
    "        vae, \n",
    "        discriminator,\n",
    "        disc_start=h_disc_start,\n",
    "        kl_weight=1e-6,\n",
    "        perc_weight=0.1,\n",
    "        disc_weight=0.05\n",
    "    ) # This loads VGG19 inside!\n",
    "\n",
    "    base_lr = 4.5e-6\n",
    "    lr = base_lr * BATCH_SIZE\n",
    "    vae_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "    disc_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    trainer.compile(vae_optimizer=vae_opt, disc_optimizer=disc_opt)\n",
    "    \n",
    "    # ... Training loop would go here ...\n",
    "    print(\"[VAE Training] Done (Placeholder). Clearing memory...\")\n",
    "    \n",
    "    # Cleanup heavy training objects\n",
    "    del trainer\n",
    "    del discriminator\n",
    "    del vae_opt\n",
    "    del disc_opt\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"[VAE Training] Memory cleared.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_training():\n",
    "    \"\"\"Main Diffusion Training Loop\"\"\"\n",
    "    \n",
    "    # 1. Preprocessing (Optional, ensures clean state)\n",
    "    run_preprocessing()\n",
    "    \n",
    "    # 2. Load Dataset\n",
    "    BATCH_SIZE = 16\n",
    "    print(\"[Main] Loading Dataset...\")\n",
    "    dataset, train_data, valid_data = input_pipeline.dataset_generator(\n",
    "        train_path, BATCH_SIZE, 0.8, save_embeding_path, ROOT\n",
    "    )\n",
    "    \n",
    "    # 3. Load VAE (Needed for Diffusion Training execution - encoding images)\n",
    "    print(\"[Main] Loading VAE for Diffusion...\")\n",
    "    vae, encoder_model, decoder_model = get_vae_models()\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.trainable = False\n",
    "    encoder_model.trainable = False\n",
    "    decoder_model.trainable = False\n",
    "    \n",
    "    # 4. Initialize UNet\n",
    "    print(\"[Main] Initializing UNet...\")\n",
    "    unet = UNetModelSmall()\n",
    "    ema_unet = UNetModelSmall()\n",
    "\n",
    "    # Build UNet\n",
    "    x_in = keras.Input(shape=(16, 16, 4))\n",
    "    t_in = keras.Input(shape=(512,))\n",
    "    c_in = keras.Input(shape=(77, 768))\n",
    "    unet([x_in, t_in, c_in])\n",
    "    ema_unet([x_in, t_in, c_in])\n",
    "    \n",
    "    print(\"[Main] Copying EMA weights...\")\n",
    "    ema_unet.set_weights(unet.get_weights())\n",
    "    \n",
    "    # 5. Initialize Diffusion Trainer\n",
    "    # Note: We pass encoder_model/decoder_model (the wrapped Keras models) or the raw layers?\n",
    "    # Checking trainer.py: self.vae_encoder(images) -> expects Keras Model or Layer\n",
    "    diffusion_model = LatentDiffusionTrainer(unet, ema_unet, encoder_model, decoder_model)\n",
    "    \n",
    "    TOTAL_EPOCHS = 50\n",
    "    EPOCH_STEPS = 460 # Adjust based on dataset size\n",
    "    TOTAL_STEPS = TOTAL_EPOCHS * EPOCH_STEPS\n",
    "    WARMUP_RATIO = 0.1\n",
    "    WARMUP_STEPS = int(TOTAL_STEPS * WARMUP_RATIO)\n",
    "    INITIAL_LR = 1e-4\n",
    "\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=INITIAL_LR,\n",
    "        decay_steps=TOTAL_STEPS, \n",
    "        warmup_steps=WARMUP_STEPS\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule, \n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    diffusion_model.compile(optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    checkpoint_path = f\"./checkpoints/tf_checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_kid\", # Monitor KID metric\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "    # æ–°çš„æ¢ä»¶å¼ç¹ªåœ– Callback\n",
    "    def conditional_plot(epoch, logs):\n",
    "        # å¾ logs å–å¾—é©—è­‰é›†çš„ KID åˆ†æ•¸\n",
    "        # æ³¨æ„ï¼šKeras çš„é©—è­‰é›† metric é€šå¸¸æœƒåŠ ä¸Š \"val_\" å‰ç¶´\n",
    "        current_kid = logs.get(\"val_kid\")\n",
    "        \n",
    "        # æ‰“å°ç•¶å‰ KID æ–¹ä¾¿ç¢ºèª\n",
    "        print(f\"\\nEpoch {epoch + 1}: val_kid = {current_kid:.4f}\")\n",
    "\n",
    "        # è¨­å®šä½ çš„é–¾å€¼ (ä¾‹å¦‚ < 1.0 æˆ–æ›´ä½ï¼Œé€šå¸¸ 0.05 ä»¥ä¸‹æ‰ç®—ä¸éŒ¯)\n",
    "        TARGET_KID = 0.8 \n",
    "\n",
    "        if current_kid is not None and current_kid < TARGET_KID:\n",
    "            diffusion_model.plot_images(valid_data)\n",
    "        else:\n",
    "            print(f\"ğŸ’¤ KID ({current_kid:.4f}) still high (>= {TARGET_KID}), skipping generation to save time.\")\n",
    "\n",
    "    plot_cb = keras.callbacks.LambdaCallback(on_epoch_end=conditional_plot)\n",
    "\n",
    "    print(\"[Main] Starting Diffusion Training...\")\n",
    "    diffusion_model.fit(\n",
    "        dataset, # Should likely be train_data? The original code used 'dataset' which was full data?? \n",
    "                 # Original code: dataset, train_data, valid_data = ... ; fit(dataset, ...)\n",
    "                 # dataset from generator is full dataset with validation mapping? \n",
    "                 # Checking input_pipeline: dataset is full, train_data is partial. \n",
    "                 # Usually fit on train_data. I will use 'train_data' to be safe and correct.\n",
    "        validation_data=valid_data,\n",
    "        epochs=TOTAL_EPOCHS,\n",
    "        callbacks=[plot_cb, checkpoint_callback],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prevent memory fragmentation\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    # Run the main pipeline\n",
    "    run_diffusion_training()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
