{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Modules\n",
    "from model.vae import Encoder, Decoder, AutoencoderKL, SDPatchGANDiscriminator\n",
    "from model.unet import UNetModelSmall\n",
    "from model.text_encoder import TextEncoderWrapper\n",
    "import input_pipeline\n",
    "from trainer import VAETrainer, LatentDiffusionTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Hardware Setup\n",
    "def setup_hardware():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Restrict TensorFlow to only use the fourth GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "setup_hardware()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './dataset'\n",
    "VOCAB = '/dictionary/id2Word.npy'\n",
    "WORD2ID = '/dictionary/word2Id.npy'\n",
    "ID2WORD = '/dictionary/id2Word.npy'\n",
    "TRAIN = '/dataset/text2ImgData.pkl'\n",
    "TEST = '/dataset/testData.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = ROOT+TRAIN\n",
    "vocab_path = ROOT+VOCAB\n",
    "word2_id_path = ROOT+WORD2ID\n",
    "id2_word_path = ROOT+ID2WORD\n",
    "save_embeding_path = \"./seqemb/seq_emb_multi.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5429 vocabularies in total\n"
     ]
    }
   ],
   "source": [
    "# Load Dictionaries (Global as they are small and needed everywhere)\n",
    "try:\n",
    "    vocab = np.load(vocab_path)\n",
    "    print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "    word2Id_dict = dict(np.load(word2_id_path))\n",
    "    id2word_dict = dict(np.load(id2_word_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Dictionaries not found. Proceeding assuming they might not be needed immediately if re-generating embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing():\n",
    "    \"\"\"Run text preprocessing using CLIP. This function scopes the TextEncoderWrapper \n",
    "    so it can be garbage collected after use.\"\"\"\n",
    "\n",
    "    print(\"\\n[Preprocessing] Checking if embeddings need to be generated...\")\n",
    "    # Only run if you need to generate embeddings\n",
    "    # preprocess_captions_all(train_path, save_embeding_path, max_caption_len=5)\n",
    "    \n",
    "    # --- ä¿®æ”¹é–‹å§‹ï¼šæŠŠä¸‹é¢é€™æ®µæœƒå ±éŒ¯çš„æ¸¬è©¦ä»£ç¢¼å…¨éƒ¨è¨»è§£æ‰ ---\n",
    "    # print(\"[Preprocessing] Loading Text Encoder for testing...\")\n",
    "    # text_encoder = TextEncoderWrapper()\n",
    "    \n",
    "    # sample = ['9', '1', '82', '5', '11', '70', '20', '31', \n",
    "    #           '3', '29', '20', '2', '5427', '5427', '5427', '5427', \n",
    "    #           '5427', '5427', '5427', '5427']\n",
    "    # # æ³¨æ„ï¼šid2word_dict éœ€è¦æ˜¯å…¨å±€è®Šæ•¸æˆ–å‚³å…¥\n",
    "    # sample_emb = text_encoder.id2clip(sample, id2word_dict)\n",
    "    # print(\"Sample embedding shape:\", sample_emb.shape)\n",
    "    \n",
    "    # # Explicitly delete to be safe, though function scope handles it\n",
    "    # del text_encoder\n",
    "    # gc.collect()\n",
    "    # tf.keras.backend.clear_session()\n",
    "    # print(\"[Preprocessing] Text Encoder memory released.\\n\")\n",
    "    # --- ä¿®æ”¹çµæŸ ---\n",
    "    \n",
    "    print(\"[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae_models():\n",
    "    \"\"\"Load and return VAE (Encoder/Decoder) with weights loaded.\"\"\"\n",
    "    decoder_weights_fpath = keras.utils.get_file(\n",
    "                origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\",\n",
    "                file_hash=\"6d3c5ba91d5cc2b134da881aaa157b2d2adc648e5625560e3ed199561d0e39d5\",\n",
    "            )\n",
    "\n",
    "    encoder_weights_fpath = keras.utils.get_file(\n",
    "        origin=\"https://huggingface.co/divamgupta/stable-diffusion-tensorflow/resolve/main/encoder_newW.h5\",\n",
    "        file_hash=\"56a2578423c640746c5e90c0a789b9b11481f47497f817e65b44a1a5538af754\",\n",
    "    )\n",
    "\n",
    "    print(\"[VAE] Loading Decoder...\")\n",
    "    decoder = Decoder()\n",
    "    latent = keras.layers.Input((16,16,4))\n",
    "    decoder_model = keras.models.Model(latent, decoder(latent))\n",
    "    decoder_model.load_weights(decoder_weights_fpath)\n",
    "\n",
    "    print(\"[VAE] Loading Encoder...\")\n",
    "    encoder = Encoder()\n",
    "    inp_img = keras.layers.Input((128,128,3))\n",
    "    encoder_model = keras.models.Model(inp_img, encoder(inp_img))\n",
    "    encoder_model.load_weights(encoder_weights_fpath)\n",
    "    \n",
    "    # Return inner models directly or wrapped ones depending on use\n",
    "    # trainer.py uses 'vae' which wraps inner encoder/decoder\n",
    "    # but LatentDiffusionTrainer uses vae.encoder / vae.decoder\n",
    "    \n",
    "    vae = AutoencoderKL(encoder, decoder, latent_channels=4, scaling_factor=0.18215)\n",
    "    return vae, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vae_training(vae, BATCH_SIZE=16):\n",
    "    \"\"\"Run VAE Training. Separation allows keeping Discriminator/LPIPS ONLY in this scope.\"\"\"\n",
    "    print(\"\\n[VAE Training] Initializing Discriminator and Trainer...\")\n",
    "    \n",
    "    discriminator = SDPatchGANDiscriminator(\n",
    "        input_channels=3,\n",
    "        base_filters=64,\n",
    "        n_layers=3\n",
    "    )\n",
    "    \n",
    "    h_disc_start = 0 # Example config\n",
    "    trainer = VAETrainer(\n",
    "        vae, \n",
    "        discriminator,\n",
    "        disc_start=h_disc_start,\n",
    "        kl_weight=1e-6,\n",
    "        perc_weight=0.1,\n",
    "        disc_weight=0.05\n",
    "    ) # This loads VGG19 inside!\n",
    "\n",
    "    base_lr = 4.5e-6\n",
    "    lr = base_lr * BATCH_SIZE\n",
    "    vae_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "    disc_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    trainer.compile(vae_optimizer=vae_opt, disc_optimizer=disc_opt)\n",
    "    \n",
    "    # ... Training loop would go here ...\n",
    "    print(\"[VAE Training] Done (Placeholder). Clearing memory...\")\n",
    "    \n",
    "    # Cleanup heavy training objects\n",
    "    del trainer\n",
    "    del discriminator\n",
    "    del vae_opt\n",
    "    del disc_opt\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"[VAE Training] Memory cleared.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_training():\n",
    "    \"\"\"Main Diffusion Training Loop\"\"\"\n",
    "    \n",
    "    # 1. Preprocessing (Optional, ensures clean state)\n",
    "    run_preprocessing()\n",
    "    \n",
    "    # 2. Load Dataset\n",
    "    BATCH_SIZE = 32\n",
    "    print(\"[Main] Loading Dataset...\")\n",
    "    dataset, train_data, valid_data = input_pipeline.dataset_generator(\n",
    "        train_path, BATCH_SIZE, 0.8, save_embeding_path, ROOT\n",
    "    )\n",
    "    \n",
    "    # 3. Load VAE (Needed for Diffusion Training execution - encoding images)\n",
    "    print(\"[Main] Loading VAE for Diffusion...\")\n",
    "    vae, encoder_model, decoder_model = get_vae_models()\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.trainable = False\n",
    "    encoder_model.trainable = False\n",
    "    decoder_model.trainable = False\n",
    "    \n",
    "    # 4. Initialize UNet\n",
    "    print(\"[Main] Initializing UNet...\")\n",
    "    unet = UNetModelSmall()\n",
    "    ema_unet = UNetModelSmall()\n",
    "\n",
    "    # Build UNet\n",
    "    x_in = keras.Input(shape=(16, 16, 4))\n",
    "    t_in = keras.Input(shape=(512,))\n",
    "    c_in = keras.Input(shape=(77, 768))\n",
    "    unet([x_in, t_in, c_in])\n",
    "    ema_unet([x_in, t_in, c_in])\n",
    "    \n",
    "    print(\"[Main] Copying EMA weights...\")\n",
    "    ema_unet.set_weights(unet.get_weights())\n",
    "    \n",
    "    # 5. Initialize Diffusion Trainer\n",
    "    # Note: We pass encoder_model/decoder_model (the wrapped Keras models) or the raw layers?\n",
    "    # Checking trainer.py: self.vae_encoder(images) -> expects Keras Model or Layer\n",
    "    diffusion_model = LatentDiffusionTrainer(unet, ema_unet, encoder_model, decoder_model)\n",
    "    \n",
    "    TOTAL_EPOCHS = 75\n",
    "    EPOCH_STEPS = 460 # Adjust based on dataset size\n",
    "    TOTAL_STEPS = TOTAL_EPOCHS * EPOCH_STEPS\n",
    "    WARMUP_RATIO = 0.1\n",
    "    WARMUP_STEPS = int(TOTAL_STEPS * WARMUP_RATIO)\n",
    "    INITIAL_LR = 1e-4\n",
    "\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=INITIAL_LR,\n",
    "        decay_steps=TOTAL_STEPS, \n",
    "        warmup_steps=WARMUP_STEPS\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule, \n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    diffusion_model.compile(optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    checkpoint_path = f\"./checkpoints/tf_checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_kid\", # Monitor KID metric\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "    # æ–°çš„æ¢ä»¶å¼ç¹ªåœ– Callback\n",
    "    def conditional_plot(epoch, logs):\n",
    "        # å¾ logs å–å¾—é©—è­‰é›†çš„ KID åˆ†æ•¸\n",
    "        # æ³¨æ„ï¼šKeras çš„é©—è­‰é›† metric é€šå¸¸æœƒåŠ ä¸Š \"val_\" å‰ç¶´\n",
    "        current_kid = logs.get(\"val_kid\")\n",
    "        \n",
    "        # æ‰“å°ç•¶å‰ KID æ–¹ä¾¿ç¢ºèª\n",
    "        print(f\"\\nEpoch {epoch + 1}: val_kid = {current_kid:.4f}\")\n",
    "\n",
    "        # è¨­å®šä½ çš„é–¾å€¼ (ä¾‹å¦‚ < 1.0 æˆ–æ›´ä½ï¼Œé€šå¸¸ 0.05 ä»¥ä¸‹æ‰ç®—ä¸éŒ¯)\n",
    "        TARGET_KID = 0.8 \n",
    "\n",
    "        if current_kid is not None and current_kid < TARGET_KID:\n",
    "            diffusion_model.plot_images(valid_data)\n",
    "        else:\n",
    "            print(f\"KID ({current_kid:.4f}) still high (>= {TARGET_KID}), skipping generation to save time.\")\n",
    "\n",
    "    plot_cb = keras.callbacks.LambdaCallback(on_epoch_end=conditional_plot)\n",
    "\n",
    "    print(\"[Main] Starting Diffusion Training...\")\n",
    "    diffusion_model.fit(\n",
    "        dataset, # Should likely be train_data? The original code used 'dataset' which was full data?? \n",
    "                 # Original code: dataset, train_data, valid_data = ... ; fit(dataset, ...)\n",
    "                 # dataset from generator is full dataset with validation mapping? \n",
    "                 # Checking input_pipeline: dataset is full, train_data is partial. \n",
    "                 # Usually fit on train_data. I will use 'train_data' to be safe and correct.\n",
    "        validation_data=valid_data,\n",
    "        epochs=TOTAL_EPOCHS,\n",
    "        callbacks=[plot_cb, checkpoint_callback],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Preprocessing] Checking if embeddings need to be generated...\n",
      "[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\n",
      "[Main] Loading Dataset...\n",
      "[Main] Loading VAE for Diffusion...\n",
      "[VAE] Loading Decoder...\n",
      "[VAE] Loading Encoder...\n",
      "[Main] Initializing UNet...\n",
      "[Main] Copying EMA weights...\n",
      "[Main] Starting Diffusion Training...\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764965677.668100  163058 service.cc:145] XLA service 0x73da03a0dbd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764965677.668148  163058 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1764965677.770966  163058 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - ETA: 0s - n_loss: 0.3878 - i_loss: 7.2192       \n",
      "Epoch 1: val_kid = 2.6901\n",
      "ğŸ’¤ KID (2.6901) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 669s 2s/step - n_loss: 0.3878 - i_loss: 7.2192 - val_n_loss: 0.2726 - val_i_loss: 2.1558 - val_kid: 2.6901\n",
      "Epoch 2/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2429 - i_loss: 1.0841   \n",
      "Epoch 2: val_kid = 1.7959\n",
      "ğŸ’¤ KID (1.7959) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 103s 412ms/step - n_loss: 0.2429 - i_loss: 1.0841 - val_n_loss: 0.2447 - val_i_loss: 0.9135 - val_kid: 1.7959\n",
      "Epoch 3/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2423 - i_loss: 0.8028   \n",
      "Epoch 3: val_kid = 1.5626\n",
      "ğŸ’¤ KID (1.5626) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 99s 403ms/step - n_loss: 0.2423 - i_loss: 0.8028 - val_n_loss: 0.2402 - val_i_loss: 0.7877 - val_kid: 1.5626\n",
      "Epoch 4/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2347 - i_loss: 0.6915   \n",
      "Epoch 4: val_kid = 1.4632\n",
      "ğŸ’¤ KID (1.4632) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 95s 402ms/step - n_loss: 0.2347 - i_loss: 0.6915 - val_n_loss: 0.2336 - val_i_loss: 0.5924 - val_kid: 1.4632\n",
      "Epoch 5/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2321 - i_loss: 0.6576   \n",
      "Epoch 5: val_kid = 1.4498\n",
      "ğŸ’¤ KID (1.4498) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 95s 395ms/step - n_loss: 0.2321 - i_loss: 0.6576 - val_n_loss: 0.2254 - val_i_loss: 0.6021 - val_kid: 1.4498\n",
      "Epoch 6/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2276 - i_loss: 0.6264   \n",
      "Epoch 6: val_kid = 1.3975\n",
      "ğŸ’¤ KID (1.3975) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 94s 394ms/step - n_loss: 0.2276 - i_loss: 0.6264 - val_n_loss: 0.2291 - val_i_loss: 0.5346 - val_kid: 1.3975\n",
      "Epoch 7/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2258 - i_loss: 0.5407   \n",
      "Epoch 7: val_kid = 1.4155\n",
      "ğŸ’¤ KID (1.4155) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 88s 372ms/step - n_loss: 0.2258 - i_loss: 0.5407 - val_n_loss: 0.2261 - val_i_loss: 0.5187 - val_kid: 1.4155\n",
      "Epoch 8/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2236 - i_loss: 0.5405   \n",
      "Epoch 8: val_kid = 1.4664\n",
      "ğŸ’¤ KID (1.4664) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 90s 373ms/step - n_loss: 0.2236 - i_loss: 0.5405 - val_n_loss: 0.2250 - val_i_loss: 0.4861 - val_kid: 1.4664\n",
      "Epoch 9/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2244 - i_loss: 0.5470   \n",
      "Epoch 9: val_kid = 1.4334\n",
      "ğŸ’¤ KID (1.4334) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 89s 372ms/step - n_loss: 0.2244 - i_loss: 0.5470 - val_n_loss: 0.2204 - val_i_loss: 0.4689 - val_kid: 1.4334\n",
      "Epoch 10/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2295 - i_loss: 0.5163   \n",
      "Epoch 10: val_kid = 1.4330\n",
      "ğŸ’¤ KID (1.4330) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 88s 370ms/step - n_loss: 0.2295 - i_loss: 0.5163 - val_n_loss: 0.2211 - val_i_loss: 0.4862 - val_kid: 1.4330\n",
      "Epoch 11/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2223 - i_loss: 0.5142   \n",
      "Epoch 11: val_kid = 1.3740\n",
      "ğŸ’¤ KID (1.3740) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 95s 397ms/step - n_loss: 0.2223 - i_loss: 0.5142 - val_n_loss: 0.2192 - val_i_loss: 0.4727 - val_kid: 1.3740\n",
      "Epoch 12/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2222 - i_loss: 0.4989   \n",
      "Epoch 12: val_kid = 1.3967\n",
      "ğŸ’¤ KID (1.3967) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 89s 371ms/step - n_loss: 0.2222 - i_loss: 0.4989 - val_n_loss: 0.2204 - val_i_loss: 0.4603 - val_kid: 1.3967\n",
      "Epoch 13/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2217 - i_loss: 0.5057   \n",
      "Epoch 13: val_kid = 1.3709\n",
      "ğŸ’¤ KID (1.3709) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 93s 395ms/step - n_loss: 0.2217 - i_loss: 0.5057 - val_n_loss: 0.2176 - val_i_loss: 0.4420 - val_kid: 1.3709\n",
      "Epoch 14/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2224 - i_loss: 0.4931   \n",
      "Epoch 14: val_kid = 1.4017\n",
      "ğŸ’¤ KID (1.4017) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 89s 373ms/step - n_loss: 0.2224 - i_loss: 0.4931 - val_n_loss: 0.2230 - val_i_loss: 0.4208 - val_kid: 1.4017\n",
      "Epoch 15/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2215 - i_loss: 0.4770   \n",
      "Epoch 15: val_kid = 1.3955\n",
      "ğŸ’¤ KID (1.3955) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 91s 379ms/step - n_loss: 0.2215 - i_loss: 0.4770 - val_n_loss: 0.2130 - val_i_loss: 0.3915 - val_kid: 1.3955\n",
      "Epoch 16/75\n",
      " 48/230 [=====>........................] - ETA: 46s - n_loss: 0.2166 - i_loss: 0.4777  \n",
      "Epoch 16: val_kid = 1.3321\n",
      "ğŸ’¤ KID (1.3321) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 93s 395ms/step - n_loss: 0.2223 - i_loss: 0.4646 - val_n_loss: 0.2224 - val_i_loss: 0.4389 - val_kid: 1.3321\n",
      "Epoch 17/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2186 - i_loss: 0.4458   \n",
      "Epoch 17: val_kid = 1.3168\n",
      "ğŸ’¤ KID (1.3168) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 94s 396ms/step - n_loss: 0.2186 - i_loss: 0.4458 - val_n_loss: 0.2170 - val_i_loss: 0.4083 - val_kid: 1.3168\n",
      "Epoch 18/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2171 - i_loss: 0.4505   \n",
      "Epoch 18: val_kid = 1.3296\n",
      "ğŸ’¤ KID (1.3296) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 89s 372ms/step - n_loss: 0.2171 - i_loss: 0.4505 - val_n_loss: 0.2194 - val_i_loss: 0.3873 - val_kid: 1.3296\n",
      "Epoch 19/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2178 - i_loss: 0.4509   \n",
      "Epoch 19: val_kid = 1.2805\n",
      "ğŸ’¤ KID (1.2805) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 93s 395ms/step - n_loss: 0.2178 - i_loss: 0.4509 - val_n_loss: 0.2175 - val_i_loss: 0.3947 - val_kid: 1.2805\n",
      "Epoch 20/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2168 - i_loss: 0.4368   \n",
      "Epoch 20: val_kid = 1.3471\n",
      "ğŸ’¤ KID (1.3471) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 89s 372ms/step - n_loss: 0.2168 - i_loss: 0.4368 - val_n_loss: 0.2168 - val_i_loss: 0.4036 - val_kid: 1.3471\n",
      "Epoch 21/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2181 - i_loss: 0.4313   \n",
      "Epoch 21: val_kid = 1.2654\n",
      "ğŸ’¤ KID (1.2654) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 94s 394ms/step - n_loss: 0.2181 - i_loss: 0.4313 - val_n_loss: 0.2172 - val_i_loss: 0.3778 - val_kid: 1.2654\n",
      "Epoch 22/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2149 - i_loss: 0.4326   \n",
      "Epoch 22: val_kid = 1.3091\n",
      "ğŸ’¤ KID (1.3091) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 88s 372ms/step - n_loss: 0.2149 - i_loss: 0.4326 - val_n_loss: 0.2149 - val_i_loss: 0.3803 - val_kid: 1.3091\n",
      "Epoch 23/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2125 - i_loss: 0.4307   \n",
      "Epoch 23: val_kid = 1.2248\n",
      "ğŸ’¤ KID (1.2248) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 94s 397ms/step - n_loss: 0.2125 - i_loss: 0.4307 - val_n_loss: 0.2102 - val_i_loss: 0.3855 - val_kid: 1.2248\n",
      "Epoch 24/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2102 - i_loss: 0.4327   \n",
      "Epoch 24: val_kid = 1.2336\n",
      "ğŸ’¤ KID (1.2336) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 88s 370ms/step - n_loss: 0.2102 - i_loss: 0.4327 - val_n_loss: 0.2193 - val_i_loss: 0.3751 - val_kid: 1.2336\n",
      "Epoch 25/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2116 - i_loss: 0.4200   \n",
      "Epoch 25: val_kid = 1.2560\n",
      "ğŸ’¤ KID (1.2560) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 90s 382ms/step - n_loss: 0.2116 - i_loss: 0.4200 - val_n_loss: 0.2088 - val_i_loss: 0.3655 - val_kid: 1.2560\n",
      "Epoch 26/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2123 - i_loss: 0.4232   \n",
      "Epoch 26: val_kid = 1.1969\n",
      "ğŸ’¤ KID (1.1969) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 94s 396ms/step - n_loss: 0.2123 - i_loss: 0.4232 - val_n_loss: 0.2099 - val_i_loss: 0.3699 - val_kid: 1.1969\n",
      "Epoch 27/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2095 - i_loss: 0.4046   \n",
      "Epoch 27: val_kid = 1.2254\n",
      "ğŸ’¤ KID (1.2254) still high (>= 0.8), skipping generation to save time.\n",
      "230/230 [==============================] - 88s 370ms/step - n_loss: 0.2095 - i_loss: 0.4046 - val_n_loss: 0.2170 - val_i_loss: 0.3560 - val_kid: 1.2254\n",
      "Epoch 28/75\n",
      "230/230 [==============================] - ETA: 0s - n_loss: 0.2142 - i_loss: 0.4048   \n",
      "Epoch 28: val_kid = 1.1905\n",
      "ğŸ’¤ KID (1.1905) still high (>= 0.8), skipping generation to save time.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "230/230 [==============================] - 93s 394ms/step - n_loss: 0.2142 - i_loss: 0.4048 - val_n_loss: 0.2024 - val_i_loss: 0.3692 - val_kid: 1.1905\n",
      "Epoch 29/75\n",
      "193/230 [========================>.....] - ETA: 9s - n_loss: 0.2109 - i_loss: 0.4016   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prevent memory fragmentation\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    # Run the main pipeline\n",
    "    run_diffusion_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
