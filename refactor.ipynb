{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Modules\n",
    "from model.vae import Encoder, Decoder, AutoencoderKL, SDPatchGANDiscriminator\n",
    "from model.unet import UNetModelSmall\n",
    "from model.text_encoder import TextEncoderWrapper\n",
    "import input_pipeline\n",
    "from trainer import VAETrainer, LatentDiffusionTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware Setup\n",
    "def setup_hardware():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Restrict TensorFlow to only use the fourth GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "setup_hardware()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './dataset'\n",
    "VOCAB = '/dictionary/id2Word.npy'\n",
    "WORD2ID = '/dictionary/word2Id.npy'\n",
    "ID2WORD = '/dictionary/id2Word.npy'\n",
    "TRAIN = '/dataset/text2ImgData.pkl'\n",
    "TEST = '/dataset/testData.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = ROOT+TRAIN\n",
    "vocab_path = ROOT+VOCAB\n",
    "word2_id_path = ROOT+WORD2ID\n",
    "id2_word_path = ROOT+ID2WORD\n",
    "save_embeding_path = \"./seqemb/seq_emb_multi.npy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dictionaries (Global as they are small and needed everywhere)\n",
    "try:\n",
    "    vocab = np.load(vocab_path)\n",
    "    print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "    word2Id_dict = dict(np.load(word2_id_path))\n",
    "    id2word_dict = dict(np.load(id2_word_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Dictionaries not found. Proceeding assuming they might not be needed immediately if re-generating embeddings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing():\n",
    "    \"\"\"Run text preprocessing using CLIP. This function scopes the TextEncoderWrapper \n",
    "    so it can be garbage collected after use.\"\"\"\n",
    "\n",
    "    print(\"\\n[Preprocessing] Checking if embeddings need to be generated...\")\n",
    "    # Only run if you need to generate embeddings\n",
    "    # preprocess_captions_all(train_path, save_embeding_path, max_caption_len=5)\n",
    "    \n",
    "    # print(\"[Preprocessing] Loading Text Encoder for testing...\")\n",
    "    # text_encoder = TextEncoderWrapper()\n",
    "    \n",
    "    # sample = ['9', '1', '82', '5', '11', '70', '20', '31', \n",
    "    #           '3', '29', '20', '2', '5427', '5427', '5427', '5427', \n",
    "    #           '5427', '5427', '5427', '5427']\n",
    "    # # 注意：id2word_dict 需要是全局變數或傳入\n",
    "    # sample_emb = text_encoder.id2clip(sample, id2word_dict)\n",
    "    # print(\"Sample embedding shape:\", sample_emb.shape)\n",
    "    \n",
    "    # # Explicitly delete to be safe, though function scope handles it\n",
    "    # del text_encoder\n",
    "    # gc.collect()\n",
    "    # tf.keras.backend.clear_session()\n",
    "    # print(\"[Preprocessing] Text Encoder memory released.\\n\")\n",
    "    \n",
    "    print(\"[Preprocessing] Skipped live encoding test (using pre-computed embeddings).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae_models():\n",
    "    \"\"\"Load and return VAE (Encoder/Decoder) with weights loaded.\"\"\"\n",
    "    decoder_weights_fpath = keras.utils.get_file(\n",
    "                origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/decoder.h5\",\n",
    "                file_hash=\"6d3c5ba91d5cc2b134da881aaa157b2d2adc648e5625560e3ed199561d0e39d5\",\n",
    "            )\n",
    "\n",
    "    encoder_weights_fpath = keras.utils.get_file(\n",
    "        origin=\"https://huggingface.co/divamgupta/stable-diffusion-tensorflow/resolve/main/encoder_newW.h5\",\n",
    "        file_hash=\"56a2578423c640746c5e90c0a789b9b11481f47497f817e65b44a1a5538af754\",\n",
    "    )\n",
    "\n",
    "    print(\"[VAE] Loading Decoder...\")\n",
    "    decoder = Decoder()\n",
    "    latent = keras.layers.Input((16,16,4))\n",
    "    decoder_model = keras.models.Model(latent, decoder(latent))\n",
    "    decoder_model.load_weights(decoder_weights_fpath)\n",
    "\n",
    "    print(\"[VAE] Loading Encoder...\")\n",
    "    encoder = Encoder()\n",
    "    inp_img = keras.layers.Input((128,128,3))\n",
    "    encoder_model = keras.models.Model(inp_img, encoder(inp_img))\n",
    "    encoder_model.load_weights(encoder_weights_fpath)\n",
    "    \n",
    "    # Return inner models directly or wrapped ones depending on use\n",
    "    # trainer.py uses 'vae' which wraps inner encoder/decoder\n",
    "    # but LatentDiffusionTrainer uses vae.encoder / vae.decoder\n",
    "    \n",
    "    vae = AutoencoderKL(encoder, decoder, latent_channels=4, scaling_factor=0.18215)\n",
    "    return vae, encoder_model, decoder_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vae_training(vae, BATCH_SIZE=16):\n",
    "    \"\"\"Run VAE Training. Separation allows keeping Discriminator/LPIPS ONLY in this scope.\"\"\"\n",
    "    print(\"\\n[VAE Training] Initializing Discriminator and Trainer...\")\n",
    "    \n",
    "    discriminator = SDPatchGANDiscriminator(\n",
    "        input_channels=3,\n",
    "        base_filters=64,\n",
    "        n_layers=3\n",
    "    )\n",
    "    \n",
    "    h_disc_start = 0 # Example config\n",
    "    trainer = VAETrainer(\n",
    "        vae, \n",
    "        discriminator,\n",
    "        disc_start=h_disc_start,\n",
    "        kl_weight=1e-6,\n",
    "        perc_weight=0.1,\n",
    "        disc_weight=0.05\n",
    "    ) # This loads VGG19 inside!\n",
    "\n",
    "    base_lr = 4.5e-6\n",
    "    lr = base_lr * BATCH_SIZE\n",
    "    vae_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "    disc_opt = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    trainer.compile(vae_optimizer=vae_opt, disc_optimizer=disc_opt)\n",
    "    \n",
    "    # ... Training loop would go here ...\n",
    "    print(\"[VAE Training] Done (Placeholder). Clearing memory...\")\n",
    "    \n",
    "    # Cleanup heavy training objects\n",
    "    del trainer\n",
    "    del discriminator\n",
    "    del vae_opt\n",
    "    del disc_opt\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"[VAE Training] Memory cleared.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_training(batch_size = 32, total_epochs = 50, continue_training = False):\n",
    "    \"\"\"Main Diffusion Training Loop\"\"\"\n",
    "    \n",
    "    # 1. Preprocessing (Optional, ensures clean state)\n",
    "    run_preprocessing()\n",
    "    \n",
    "    # 2. Load Dataset\n",
    "    print(\"[Main] Loading Dataset...\")\n",
    "    dataset, train_data, valid_data = input_pipeline.dataset_generator(\n",
    "        train_path, batch_size, 0.8, save_embeding_path, ROOT\n",
    "    )\n",
    "    \n",
    "    # 3. Load VAE (Needed for Diffusion Training execution - encoding images)\n",
    "    print(\"[Main] Loading VAE for Diffusion...\")\n",
    "    vae, encoder_model, decoder_model = get_vae_models()\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.trainable = False\n",
    "    encoder_model.trainable = False\n",
    "    decoder_model.trainable = False\n",
    "    \n",
    "    # 4. Initialize UNet\n",
    "    print(\"[Main] Initializing UNet...\")\n",
    "    unet = UNetModelSmall()\n",
    "    ema_unet = UNetModelSmall()\n",
    "\n",
    "    # Build UNet\n",
    "    x_in = keras.Input(shape=(16, 16, 4))\n",
    "    t_in = keras.Input(shape=(512,))\n",
    "    c_in = keras.Input(shape=(77, 768))\n",
    "    unet([x_in, t_in, c_in])\n",
    "    ema_unet([x_in, t_in, c_in])\n",
    "    \n",
    "    print(\"[Main] Copying EMA weights...\")\n",
    "    ema_unet.set_weights(unet.get_weights())\n",
    "    \n",
    "    # 5. Initialize Diffusion Trainer\n",
    "    # Note: We pass encoder_model/decoder_model (the wrapped Keras models) or the raw layers?\n",
    "    # Checking trainer.py: self.vae_encoder(images) -> expects Keras Model or Layer\n",
    "    diffusion_model = LatentDiffusionTrainer(unet, ema_unet, encoder_model, decoder_model)\n",
    "    \n",
    "    EPOCH_STEPS = 460 # Adjust based on dataset size\n",
    "    TOTAL_STEPS = total_epochs * EPOCH_STEPS\n",
    "    WARMUP_RATIO = 0.1\n",
    "    WARMUP_STEPS = int(TOTAL_STEPS * WARMUP_RATIO)\n",
    "    INITIAL_LR = 1e-4\n",
    "\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=INITIAL_LR,\n",
    "        decay_steps=TOTAL_STEPS, \n",
    "        warmup_steps=WARMUP_STEPS\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule, \n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8\n",
    "    )\n",
    "    \n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    diffusion_model.compile(optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    checkpoint_path = f\"./checkpoints/tf_checkpoint.weights.h5\"\n",
    "\n",
    "    # 檢查並載入權重\n",
    "    if continue_training and os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint at {checkpoint_path}, loading weights to resume training...\")\n",
    "        \n",
    "        # 強制 Build KID\n",
    "        try:\n",
    "            print(\"Force building KID layer to match checkpoint structure...\")\n",
    "            # 建立假圖片數據 (Batch=1, Size=128x128, Channels=3)\n",
    "            dummy_img = tf.zeros((1, 128, 128, 3))\n",
    "            # 戳一下 update_state，強迫 InceptionV3 初始化權重\n",
    "            diffusion_model.kid.update_state(dummy_img, dummy_img)\n",
    "            print(\"KID layer built successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to force build KID: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # 因為是 save_weights_only=True，所以用 load_weights\n",
    "            diffusion_model.load_weights(checkpoint_path, skip_mismatch=True)\n",
    "            print(\"Weights loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load weights: {e}\")\n",
    "            print(\"Starting training from scratch.\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_kid\", # Monitor KID metric\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "    # 新的條件式繪圖 Callback\n",
    "    def conditional_plot(epoch, logs):\n",
    "        # 從 logs 取得驗證集的 KID 分數\n",
    "        current_kid = logs.get(\"val_kid\")\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}: val_kid = {current_kid:.4f}\")\n",
    "\n",
    "        # 設定你的閾值 (例如 < 1.0 或更低)\n",
    "        TARGET_KID = 0.8 \n",
    "\n",
    "        if current_kid is not None and current_kid < TARGET_KID:\n",
    "            diffusion_model.plot_images(valid_data)\n",
    "        else:\n",
    "            print(f\"KID ({current_kid:.4f}) still high (>= {TARGET_KID}), skipping generation to save time.\")\n",
    "\n",
    "    plot_cb = keras.callbacks.LambdaCallback(on_epoch_end=conditional_plot)\n",
    "\n",
    "    print(\"[Main] Starting Diffusion Training...\")\n",
    "    diffusion_model.fit(\n",
    "        dataset, \n",
    "        validation_data=valid_data,\n",
    "        epochs=total_epochs,\n",
    "        callbacks=[plot_cb, checkpoint_callback],\n",
    "        verbose=2\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prevent memory fragmentation\n",
    "    os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    TOTAL_EPOCHS = 50\n",
    "    # Run the main pipeline\n",
    "    run_diffusion_training(BATCH_SIZE, TOTAL_EPOCHS, continue_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
